{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_occurence_matrix(text, target=None, stop_words=None, binary=True, preprocess_text=False):  \n",
    "    '''\n",
    "    Output is messages x (unique) words\n",
    "    \n",
    "    If binary=True, then each element represents if the word is in the message or not.\n",
    "    Otherwise, it represents the count of how many times that word appears in that message.\n",
    "    ''' \n",
    "    if target:\n",
    "        text = list(filter(lambda x : target in x, text)) #Filter comments in which target word is present\n",
    "        \n",
    "    preprocessor = CountVectorizer(strip_accents='unicode').build_preprocessor()   \n",
    "    if stop_words:        \n",
    "        stop_words = [preprocessor(word) for word in stop_words] #preprocesses stop words\n",
    "    if preprocess_text:\n",
    "        text = [preprocessor(msg) for msg in text] #preprocesses text\n",
    "        \n",
    "    #calculates word count for each message\n",
    "    vectorizer = CountVectorizer(strip_accents='unicode', stop_words=stop_words, binary=binary)\n",
    "    X = vectorizer.fit_transform(text).toarray()\n",
    "    \n",
    "    labels = vectorizer.get_feature_names()\n",
    "    \n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('../comentarios_sorted_votes.csv')\n",
    "stop_words = [word.rstrip() for word in open('stopwords.txt')]\n",
    "\n",
    "#Preprocesses text\n",
    "preprocessor = CountVectorizer(strip_accents='unicode').build_preprocessor() #lowercase and strip accents\n",
    "stop_words = [preprocessor(word) for word in stop_words]\n",
    "comments['text'] = [preprocessor(msg) for msg in comments['text']]\n",
    "comments['text'] = [' '.join([word for word in RegexpTokenizer(r'\\w+').tokenize(msg) if not word in stop_words])\n",
    "                    for msg in comments['text']]\n",
    "\n",
    "N = 30 #Consider only the N most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_by_channel = []\n",
    "channels = []\n",
    "for channel, group in comments.groupby('uploader'):\n",
    "    channel_comments = ' '.join(group['text'])\n",
    "    comments_by_channel.append(channel_comments)\n",
    "    channels.append(channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencia relativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Em relação ao total\n",
    "Aqui, foi usada a distribuição de probabilidade conjunta das palavras e canais, i.e., $p(a_i, c_j) = P(A=a_i, C= C_j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequência relativa ao total:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dr. Alain Dutra</th>\n",
       "      <th>Dr. Alvaro Galvão</th>\n",
       "      <th>Dr. Felipe Ades MD PhD</th>\n",
       "      <th>Dr. Fernando Gomes</th>\n",
       "      <th>Dr. Lair Ribeiro Oficial</th>\n",
       "      <th>Drauzio Varella</th>\n",
       "      <th>Julio Pereira - Neurocirurgião</th>\n",
       "      <th>Lucy Kerr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>私も乳癌になり先生方に助けて頂き今かあり感謝</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>賢者</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>身体に</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>隼hayabusa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>頑張って下さい</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Dr. Alain Dutra  Dr. Alvaro Galvão  \\\n",
       "私も乳癌になり先生方に助けて頂き今かあり感謝              0.0                0.0   \n",
       "賢者                                  0.0                0.0   \n",
       "身体に                                 0.0                0.0   \n",
       "隼hayabusa                           0.0                0.0   \n",
       "頑張って下さい                             0.0                0.0   \n",
       "\n",
       "                        Dr. Felipe Ades MD PhD  Dr. Fernando Gomes  \\\n",
       "私も乳癌になり先生方に助けて頂き今かあり感謝                     0.0            0.000001   \n",
       "賢者                                         0.0            0.000000   \n",
       "身体に                                        0.0            0.000001   \n",
       "隼hayabusa                                  0.0            0.000000   \n",
       "頑張って下さい                                    0.0            0.000001   \n",
       "\n",
       "                        Dr. Lair Ribeiro Oficial  Drauzio Varella  \\\n",
       "私も乳癌になり先生方に助けて頂き今かあり感謝                       0.0              0.0   \n",
       "賢者                                           0.0              0.0   \n",
       "身体に                                          0.0              0.0   \n",
       "隼hayabusa                                    0.0              0.0   \n",
       "頑張って下さい                                      0.0              0.0   \n",
       "\n",
       "                        Julio Pereira - Neurocirurgião  Lucy Kerr  \n",
       "私も乳癌になり先生方に助けて頂き今かあり感謝                        0.000000        0.0  \n",
       "賢者                                            0.000001        0.0  \n",
       "身体に                                           0.000000        0.0  \n",
       "隼hayabusa                                     0.000007        0.0  \n",
       "頑張って下さい                                       0.000000        0.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, labels = word_occurence_matrix(comments_by_channel, stop_words=stop_words, binary=False)\n",
    "\n",
    "X = X / X.sum()\n",
    "print('Frequência relativa ao total:')\n",
    "df_total = pd.DataFrame(X.T, columns = channels, index = labels)\n",
    "df_total.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 palavras com maior desvio padrão entre os canais:\n",
      "\n",
      "[('dias', '0.0016'), ('pra', '0.0012'), ('cheiro', '0.0012'), ('olfato', '0.0011'), ('paladar', '0.0011'), ('drauzio', '0.0011'), ('deus', '0.0011'), ('dr', '0.0009'), ('vc', '0.0009'), ('virus', '0.0009'), ('dor', '0.0009'), ('video', '0.0008'), ('vai', '0.0008'), ('pessoas', '0.0007'), ('ivermectina', '0.0007'), ('sinto', '0.0007'), ('sentir', '0.0007'), ('nada', '0.0007'), ('gosto', '0.0007'), ('to', '0.0007'), ('dia', '0.0007'), ('assim', '0.0007'), ('sintomas', '0.0007'), ('agora', '0.0006'), ('covid', '0.0006'), ('bem', '0.0006'), ('ainda', '0.0006'), ('casa', '0.0005'), ('ta', '0.0005'), ('tudo', '0.0005')]\n"
     ]
    }
   ],
   "source": [
    "#Selects only the N with higher std\n",
    "word_std = X.std(axis=0)\n",
    "higher_std_zipped = sorted(zip(word_std, labels, X.T), reverse=True)[:N]\n",
    "word_std, labels, X_t = zip(*higher_std_zipped)\n",
    "\n",
    "word_std_i = ['%.4f'%(x) for x in word_std]\n",
    "print(f'{N} palavras com maior desvio padrão entre os canais:\\n')\n",
    "print(list(zip(labels, word_std_i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Em relação ao canal\n",
    "Aqui, foi obtida a distribuição de probabilidade das palavras em cada canal $C_k$, i.e., $p(a_i|C_k) = P(A=a_i|C_k)$. \n",
    "Com isso, podemos fazer corretamente comparações entre os canais. Foram, então, selecionadas as palavras cuja ocorrência mais se difere entre eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequência relativa a cada canal:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dr. Alain Dutra</th>\n",
       "      <th>Dr. Alvaro Galvão</th>\n",
       "      <th>Dr. Felipe Ades MD PhD</th>\n",
       "      <th>Dr. Fernando Gomes</th>\n",
       "      <th>Dr. Lair Ribeiro Oficial</th>\n",
       "      <th>Drauzio Varella</th>\n",
       "      <th>Julio Pereira - Neurocirurgião</th>\n",
       "      <th>Lucy Kerr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>私も乳癌になり先生方に助けて頂き今かあり感謝</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>賢者</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>身体に</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>隼hayabusa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>頑張って下さい</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Dr. Alain Dutra  Dr. Alvaro Galvão  \\\n",
       "私も乳癌になり先生方に助けて頂き今かあり感謝              0.0                0.0   \n",
       "賢者                                  0.0                0.0   \n",
       "身体に                                 0.0                0.0   \n",
       "隼hayabusa                           0.0                0.0   \n",
       "頑張って下さい                             0.0                0.0   \n",
       "\n",
       "                        Dr. Felipe Ades MD PhD  Dr. Fernando Gomes  \\\n",
       "私も乳癌になり先生方に助けて頂き今かあり感謝                     0.0            0.000001   \n",
       "賢者                                         0.0            0.000000   \n",
       "身体に                                        0.0            0.000001   \n",
       "隼hayabusa                                  0.0            0.000000   \n",
       "頑張って下さい                                    0.0            0.000001   \n",
       "\n",
       "                        Dr. Lair Ribeiro Oficial  Drauzio Varella  \\\n",
       "私も乳癌になり先生方に助けて頂き今かあり感謝                       0.0              0.0   \n",
       "賢者                                           0.0              0.0   \n",
       "身体に                                          0.0              0.0   \n",
       "隼hayabusa                                    0.0              0.0   \n",
       "頑張って下さい                                      0.0              0.0   \n",
       "\n",
       "                        Julio Pereira - Neurocirurgião  Lucy Kerr  \n",
       "私も乳癌になり先生方に助けて頂き今かあり感謝                        0.000000        0.0  \n",
       "賢者                                            0.000001        0.0  \n",
       "身体に                                           0.000000        0.0  \n",
       "隼hayabusa                                     0.000007        0.0  \n",
       "頑張って下さい                                       0.000000        0.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, labels = word_occurence_matrix(comments_by_channel, stop_words=stop_words, binary=False)\n",
    "#Makes frequencies relative to each channel so that we have the probability distribution of words for each channel\n",
    "X = (X.T / X.sum(axis=1)).T\n",
    "print('Frequência relativa a cada canal:')\n",
    "df_canal = pd.DataFrame(X.T, columns = channels, index = labels)\n",
    "df_total.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 palavras com maior desvio padrão entre os canais:\n",
      "\n",
      "[('dr', '0.0116'), ('fernando', '0.0115'), ('dias', '0.0083'), ('lair', '0.0077'), ('ivermectina', '0.0060'), ('you', '0.0054'), ('dor', '0.0046'), ('tomar', '0.0044'), ('fiz', '0.0042'), ('comprimidos', '0.0042'), ('sintomas', '0.0041'), ('olfato', '0.0041'), ('boa', '0.0040'), ('paladar', '0.0040'), ('cheiro', '0.0039'), ('deu', '0.0038'), ('covid', '0.0037'), ('noite', '0.0037'), ('15', '0.0036'), ('ribeiro', '0.0036'), ('parabens', '0.0035'), ('pinto', '0.0034'), ('dia', '0.0033'), ('gomes', '0.0033'), ('drauzio', '0.0030'), ('good', '0.0030'), ('positivo', '0.0029'), ('dra', '0.0029'), ('deus', '0.0028'), ('to', '0.0028')]\n"
     ]
    }
   ],
   "source": [
    "#Selects only the N with higher std\n",
    "word_std = X.std(axis=0)\n",
    "higher_std_zipped = sorted(zip(word_std, labels, X.T), reverse=True)[:N]\n",
    "word_std, labels, X_t = zip(*higher_std_zipped)\n",
    "\n",
    "word_std_i = ['%.4f'%(x) for x in word_std]\n",
    "print(f'{N} palavras com maior desvio padrão entre os canais:\\n')\n",
    "print(list(zip(labels, word_std_i)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (ic)",
   "language": "python",
   "name": "ic-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
