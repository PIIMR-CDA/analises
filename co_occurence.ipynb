{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/barbara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(text):\n",
    "    words = [word for message in text for word in message]\n",
    "    return set(words)\n",
    "\n",
    "def co_occurence_matrix_with_window(text, window_size, stop_words):\n",
    "    '''\n",
    "    Calculates the co-occurence matrix using a sliding window of size window_size as context    \n",
    "    Output is (unique) words x (unique) words\n",
    "    '''\n",
    "    unique_words = get_unique_words(text)\n",
    "    n = len(unique_words) #number of unique words\n",
    "    co_matrix = pd.DataFrame(data=np.zeros([n,n]), columns=unique_words, index=unique_words, dtype='int')\n",
    "\n",
    "    #for each word in each message, counts +1 for each pair [word,context]\n",
    "    for message in text:\n",
    "        msg_len = len(message)\n",
    "        for i, word in enumerate(message):\n",
    "            first = max(i - window_size, 0)\n",
    "            last = min(i + window_size + 1, msg_len)\n",
    "            for context in message[first:last]:\n",
    "                co_matrix.loc[word,context] += 1\n",
    "    return co_matrix  \n",
    "    \n",
    "def word_occurence_matrix(text, target=None, stop_words=None, binary=True, preprocess_text=False):  \n",
    "    '''\n",
    "    Output is messages x (unique) words\n",
    "    \n",
    "    If binary=True, then each element represents if the word is in the message or not.\n",
    "    Otherwise, it represents the count of how many times that word appears in that message.\n",
    "    ''' \n",
    "    if target:\n",
    "        text = list(filter(lambda x : target in x, text)) #Filter comments in which target word is present\n",
    "        \n",
    "    preprocessor = CountVectorizer(strip_accents='unicode').build_preprocessor()   \n",
    "    if stop_words:        \n",
    "        stop_words = [preprocessor(word) for word in stop_words] #preprocesses stop words\n",
    "    if preprocess_text:\n",
    "        text = [preprocessor(msg) for msg in text] #preprocesses text\n",
    "        \n",
    "    #calculates word count for each message\n",
    "    vectorizer = CountVectorizer(strip_accents='unicode', stop_words=stop_words, binary=binary)\n",
    "    X = vectorizer.fit_transform(text).toarray()\n",
    "    \n",
    "    labels = vectorizer.get_feature_names()\n",
    "    \n",
    "    return X, labels\n",
    "\n",
    "def co_occurence_matrix(word_occurence_matrix):\n",
    "    '''\n",
    "    Co-occurence matrix created based on the word count/occurence matrix.     \n",
    "    In other words, this co-occurence matrix will consider each message as the context for all words in it.\n",
    "    '''\n",
    "    co_X = word_occurence_matrix.T @ word_occurence_matrix\n",
    "    np.fill_diagonal(co_X, 0)\n",
    "    return co_X\n",
    "\n",
    "def score_ngrams(text, freq_filter=1, score_metric=BigramAssocMeasures().pmi):\n",
    "    '''\n",
    "    Score n-grams using score_metric. Defaults to PMI.\n",
    "    '''\n",
    "    words = [word for msg in text for word in msg.split()]\n",
    "    finder = BigramCollocationFinder.from_words(words)\n",
    "    finder.apply_freq_filter(freq_filter)\n",
    "    return finder.score_ngrams(score_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('../comentarios_sorted_votes.csv')\n",
    "stop_words = [word.rstrip() for word in open('stopwords.txt')]\n",
    "\n",
    "#Preprocesses text\n",
    "preprocessor = CountVectorizer(strip_accents='unicode').build_preprocessor() #lowercase and strip accents\n",
    "stop_words = [preprocessor(word) for word in stop_words]\n",
    "comments['text'] = [preprocessor(msg) for msg in comments['text']]\n",
    "comments['text'] = [' '.join([word for word in RegexpTokenizer(r'\\w+').tokenize(msg) if not word in stop_words])\n",
    "                    for msg in comments['text']]\n",
    "\n",
    "text = comments['text'].dropna()\n",
    "\n",
    "N = 30 #Consider only the N most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Co-ocorrência "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Co-cocorrência com uma palavra pré-definida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 palavras que mais ocorrem nos mesmos comentários em que \"cloroquina\" ocorre:\n",
      "\n",
      "[('cloroquina', 615), ('hidroxicloroquina', 497), ('ivermectina', 360), ('covid', 209), ('azitromicina', 163), ('medicos', 154), ('tratamento', 153), ('dr', 151), ('sobre', 149), ('uso', 147), ('pessoas', 145), ('pra', 142), ('estudo', 138), ('19', 135), ('medico', 134), ('pode', 119), ('todos', 114), ('bem', 108), ('contra', 106), ('dra', 103), ('tomar', 103), ('agora', 102), ('protocolo', 99), ('https', 96), ('porque', 96), ('sim', 95), ('sintomas', 92), ('saude', 91), ('virus', 91), ('pois', 90)]\n"
     ]
    }
   ],
   "source": [
    "target = 'cloroquina'\n",
    "\n",
    "X, labels = word_occurence_matrix(text, target=target, stop_words=stop_words)\n",
    "\n",
    "word_count = X.sum(axis=0) #no. of comments containing A in which each word appears\n",
    "word_count = sorted(zip(labels, word_count), reverse=True, key=lambda tupl : tupl[1])\n",
    "print(f'{N} palavras que mais ocorrem nos mesmos comentários em que \"{target}\" ocorre:\\n')\n",
    "print(word_count[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. N-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerações:\n",
    "\n",
    "- PMI - favorece n-gramas raros. Muitos n-gramas com a mesma score (a mais alta)\n",
    "\n",
    "#### PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 com score mais alta:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('democraticos', 'solidarios'), 14.776091834429147),\n",
       " (('98200', '1348'), 14.650560952345288),\n",
       " (('roubos', 'assedios'), 14.535083734925351),\n",
       " (('351969602263', 'valdemir'), 14.092565499224401),\n",
       " (('watsapp', '351969602263'), 14.065598451624133),\n",
       " (('pet', 'shops'), 13.748336168031768),\n",
       " (('joaquim', 'inacio'), 13.73620904325611),\n",
       " (('debaixo', 'cuberta'), 13.59166726329172),\n",
       " (('cestas', 'basicas'), 13.419606517505416),\n",
       " (('albert', 'dickson'), 13.386083129641825),\n",
       " (('121', '150'), 13.293008947727204),\n",
       " (('irmaos', 'irmas'), 13.269739168404357),\n",
       " (('sars', 'cov'), 13.226866590935995),\n",
       " (('aplicativo', 'protetor'), 13.21167671111208),\n",
       " (('botao', 'alerta'), 13.187160431505044),\n",
       " (('inimigo', 'invisivel'), 13.18553531723477),\n",
       " (('bla', 'bla'), 13.180160669201733),\n",
       " (('congestao', 'nasal'), 13.142883752420245),\n",
       " (('carol', 'bandeira'), 13.10799553979401),\n",
       " (('evite', 'roubos'), 13.055915898226793),\n",
       " (('lojas', 'agropecuarias'), 13.023155283351159),\n",
       " (('atila', 'iamarino'), 12.996806516418971),\n",
       " (('hs', 'debaixo'), 12.876644222006192),\n",
       " (('ansia', 'vomito'), 12.861179464027945),\n",
       " (('global', 'watsapp'), 12.85100440084809),\n",
       " (('macgyver', 'global'), 12.832167758835237),\n",
       " (('coreia', 'sul'), 12.82247633585666),\n",
       " (('insta', 'fonothaisprett'), 12.764864579005895),\n",
       " (('31', '98200'), 12.722453870827072),\n",
       " (('deyze', 'kelly'), 12.685603337195074)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmi = BigramAssocMeasures().pmi\n",
    "print(f'{N} com score mais alta:')\n",
    "score_ngrams(text, score_metric=pmi, freq_filter=30)[:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### likelihood ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 com score mais alta:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('covid', '19'), 16892.947470233976),\n",
       " (('falta', 'ar'), 16186.704051448698),\n",
       " (('olfato', 'paladar'), 14115.073150518603),\n",
       " (('gracas', 'deus'), 12369.146510580387),\n",
       " (('dor', 'cabeca'), 11614.437174829865),\n",
       " (('deus', 'abencoe'), 11288.143898218037),\n",
       " (('youtu', 'be'), 11039.053082917235),\n",
       " (('dra', 'lucy'), 10155.46757782443),\n",
       " (('dr', 'drauzio'), 10016.641748516702),\n",
       " (('https', 'youtu'), 8958.369513217573),\n",
       " (('sentir', 'cheiro'), 8539.500821771626),\n",
       " (('15', 'dias'), 8298.801986814038),\n",
       " (('https', 'www'), 8007.512202706735),\n",
       " (('lucy', 'kerr'), 7973.473043179992),\n",
       " (('3', 'dias'), 7645.367432256628),\n",
       " (('boa', 'noite'), 6445.995892016467),\n",
       " (('cheiro', 'gosto'), 6187.1150865093),\n",
       " (('www', 'youtube'), 6153.505077922475),\n",
       " (('corona', 'virus'), 5224.399715203055),\n",
       " (('sinto', 'cheiro'), 4572.425125205145),\n",
       " (('vitamina', 'c'), 4523.5120698652),\n",
       " (('bom', 'dia'), 4473.085106279236),\n",
       " (('3', 'comprimidos'), 4406.6050363202885),\n",
       " (('lair', 'ribeiro'), 4213.562386985365),\n",
       " (('todo', 'mundo'), 4042.2175669152293),\n",
       " (('grupo', 'risco'), 3970.3295114950315),\n",
       " (('dr', 'lair'), 3899.0367454070556),\n",
       " (('dias', 'seguidos'), 3791.36382324813),\n",
       " (('paladar', 'olfato'), 3719.7094160007055),\n",
       " (('quanto', 'tempo'), 3713.5556193457937)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = BigramAssocMeasures().likelihood_ratio\n",
    "print(f'{N} com score mais alta:')\n",
    "score_ngrams(text, score_metric=lr, freq_filter=3)[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (ic)",
   "language": "python",
   "name": "ic-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
